# clarinet â€” sax based evented streaming json parser in javascript

i'm super happy to announce `clarinet`. it's currently running ~110 [tests] both in the `browser` and in `node.js` which include some of the most obtuse edge cases i could find in other json parser tests, and can currently parse all `npm` registry without blinking.

`clarinet` is not a replacement for `json.parse`. if you can `json.parse`, you should. it's super fast, comes bundled in v8, and that's all. move along.

my motivation for `clarinet` was to stream large (or small) chunks of json data and be able to created indexes on the fly.

`clarinet` is more like `sax`. it's a streaming parser and when you feed it json chunks it will emit events. 

or in code:

``` javascript
var chunks = ['{"foo":', ' "bar', '"}'];
```

you can't parse that. even if you control the source that is emitting those chunks there's plenty of situations where you can't just emit a 10mb file in one chunk. also if your json file is larger than the memory you have available on your computer, you need `clarinet`.

this is how you would implement substack's [npmtop], a tool that returns an list on npm modules authors ordered by the number of modules they publish, with `clarinet`:

``` javascript
var fs             = require('fs')
  , clarinet       = require('clarinet')
  , parse_stream   = clarinet.createStream()
  , author         = false // was the previous key an author?
  , authors        = {}    // authors found so far
  ;

// open object is emitted when we find '{'
// the name is the first key of the json object
// subsequent ones will emit key
parse_stream.on('openobject', function(name) {
  if(name==='author') author=true;
});

// a key was found
parse_stream.on('key', function(name) {
  if(name==='author') author=true;
});

// we got all of npm, lets aggregates results 
// and sort them by repo count.
parse_stream.on('end', function () {
  var sorted = []
    , i
    ;
  for (var a in authors)
    sorted.push([a, authors[a]]);
  sorted.sort(function(a, b) { return a[1] - b[1]; });
  i = sorted.length-1;
  while(i!==-1) {
    console.log(sorted.length-i, sorted[i]);
    i--;
  }
});

// value is emitted when we find a json value, just like in the
// specification in json.org: strings, true, false, null, and number.
//
// you can find out the value type by running a typeof
//
// this could be faster if we emitted different events for each value.
// e.g. .on('string'), .on('true'), etc..
//
// would be faster cause clarinet wouldn't have to parse it for you
// but this api choice seemed easier for the developer 
// that needs to have less events
// to attend to
parse_stream.on('value', function(value) {
  if(author) { 
    // get the current count for this author
    var current_count = authors[value];
    // if it exists increment it
    if (current_count) authors[value] +=1;
    // else it's the first one
    else authors[value] = 1;
    // this is not an author key
    author=false; 
  }
});

// create a read stream and pipe it to clarinet
fs.createReadStream(__dirname + '/npm.json').pipe(parse_stream);
```

feel free to browse the [docs] and [samples] for more goodies. feedback is great, pull requests are even better.

## performance

### tl;dr

* clarinet is good and fast enough at doing what it is designed to do
* `json.parse` is way faster - you should use it if you can

since having a streaming parser requires consistently understanding performance implications, i've done a preliminary [study] on how well `clarinet` performs. [source code is open][bench] so you are welcome to replicate.

because none of the other parsers tests was able to do streaming json parsing i had to create a test that uses `fs.readfilesync` so all of the parsers could be tested. this sucks, i really wanted to test async parsers but none existed. i tried [yajl]  (a c++ module that looks a lot like clarinet) but it's current  version does not build in node `0.6`. `jsonparse` should also be able run asynchronously but since it's not documented properly and was made in a previous version of node i was unable to make it work . if you are looking for differences between `jsonparse` and `clarinet`:

* `jsonparse` emits less events and is not sax like. this makes it faster on smaller json files and much slower on large json files. plus it makes it unsuited for the certain lower level application like the one i had in mind, building indexes. both `clarinet` and `jsonparse` would benefit from abstractions on top that allow people to do json streaming for common case scenarios, even if at this point it's not obvious what that means.

if you want your parser to be included, or refute any of my claims please send me an email and i'll fix this article provided you give me source code and results to go along with it.

i created an [async] version of the tests but only clarinet is included there for obvious reasons. in the process i also created a [profiling] page that can help you get profiling information about `clarinet` using google chrome developer tools.

### in detail

in the test we've compared `clarinet`, `json.parse` (referred as v8 in the tables and figures) and @[creationix] [jsonparse]. to avoid sample bias i've tested all modules against four different json documents:

* **[npm]**: the full npm registry (11m). parsed once.
* **[twitter]**: a bunch of node.js tweets (13m). parsed once.
* **[creationix][creationixs]**: one of `jsonparse` test cases (4.5k). parsed 2000 times to have more significant times.
* **[wikipedia]**: the json sample from wikipedia.org/json (467b). parsed 100000 times to have more significant times.

in order to test whether `clarinet`, `json.parse`, and the `jsonparse` modules differed in terms of execution time, i conducted analyses of variance (anovas). to obtain the estimate data, i ran [scripts][bench] that created 10 runs for each json document, resulting in 40 measurements per parser.

then, the three modules were at first compared between each other, regardless of the documents that generated their values (i.e. the execution times). this anova showed that the differences in the execution times obtained with `clarinet`, `json.parse`, and `jsonparse`, were statistically significant (f(2,117) = 40.28, p = .000).

post-hoc tests with scheffe correction revealed that the execution times of `json.parse` module were statistically different from both `clarinet` and `jsonparse`, but these two did not differ from one another (see table 1 and fig. 1). specifically, `json.parse` module demonstrated smaller execution times than both `clarinet` and `jsonparse`.

![table 1](http://writings.nunojob.com/images/clarinet-table1.png "table 1")
![figure 1](http://writings.nunojob.com/images/clarinet-figure1.png "figure 1 overall")

next, given that two of the documents were "big" (i.e. > 1mb), and two of them were "small" (i.e. < 1mb) and the expectation that the size of the documents would play an important role in the performance of the modules, i computed an anova to compare the execution times of the modules for the "big" documents, and another anova to compare them for the "small" documents.

the differences between the execution times of `clarinet`, `json.parse`, and `jsonparse` were statistically significant for the "big" documents (f(2,57) = 279.96, p = .000).

post-hoc tests with scheffe correction revealed that the execution times of the modules were statistically different between the three of them (see table 2 and fig. 2).

![table 2](http://writings.nunojob.com/images/clarinet-table2.png "table 2")
![figure 2](http://writings.nunojob.com/images/clarinet-figure2.png "figure 2 big")

the differences between the execution times of `clarinet`, `json.parse`, and `jsonparse` were also statistically significant for the "small" documents (f(2,57) = 36.95, p = .000).

post-hoc tests with scheffe correction revealed that the execution times of the modules were once again statistically different between the three of them (see table 3 and fig. 3).

![table 3](http://writings.nunojob.com/images/clarinet-table3.png "table 3")
![figure 3](http://writings.nunojob.com/images/clarinet-figure3.png "figure 3 small")

in conclusion, the execution times of the three modules under analysis were different in all the conditions tested (i.e. regardless of the document size, for big documents only, and for small documents only), but this difference was greater when considering the estimates made for dealing with big documents only (which can be seen by the f ratios), where the `json.parse` demonstrated clearly smaller execution times.

[npmtop]: https://github.com/substack/npmtop
[docs]: https://github.com/dscape/clarinet
[samples]: https://github.com/dscape/clarinet/tree/master/samples
[tests]: https://github.com/dscape/clarinet/blob/master/test/clarinet.js
[study]: https://github.com/dscape/clarinet/tree/master/bench/results/dscape-study
[bench]: https://github.com/dscape/clarinet/tree/master/bench
[async]: https://github.com/dscape/clarinet/blob/master/bench/async.js
[creationix]: https://github.com/creationix
[jsonparse]: https://github.com/creationix/jsonparse
[twitter]: https://github.com/dscape/clarinet/blob/master/samples/twitter.json
[npm]: https://github.com/dscape/clarinet/blob/master/samples/npm.json
[wikipedia]: https://github.com/dscape/clarinet/blob/master/samples/wikipedia.json
[creationixs]: https://github.com/dscape/clarinet/blob/master/samples/creationix.json
[profiling]: https://github.com/dscape/clarinet/blob/master/test/bench.html
[yajl]: https://github.com/lloyd/node-yajl